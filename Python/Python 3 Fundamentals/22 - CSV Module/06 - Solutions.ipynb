{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electoral-wholesale",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-amplifier",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-electricity",
   "metadata": {},
   "source": [
    "Alongside this note book, four CSV files are specified (one is in fact a TSV file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-pavilion",
   "metadata": {},
   "source": [
    "For each file, load it using the CSV module, and find the smallest and largest numbers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-avenue",
   "metadata": {},
   "source": [
    "All these files contain just lists of numbers - with the exception of a possible header row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-swedish",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-photography",
   "metadata": {},
   "source": [
    "The first file `file1.csv` looks like a standard CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "id": "medieval-centre",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.093752Z",
     "start_time": "2025-06-18T04:56:35.084375Z"
    }
   },
   "source": [
    "import csv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "alternate-example",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.426636Z",
     "start_time": "2025-06-18T04:56:35.418261Z"
    }
   },
   "source": [
    "with open('file1.csv') as f:\n",
    "    for row in f:\n",
    "        print(row.strip())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1,col2,col3\n",
      "10,20,30\n",
      "30,40,50\n",
      "60,70,80\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "ahead-edinburgh",
   "metadata": {},
   "source": [
    "We should be able to load this up without any issues using a csv reader:"
   ]
  },
  {
   "cell_type": "code",
   "id": "brilliant-player",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.535658Z",
     "start_time": "2025-06-18T04:56:35.529957Z"
    }
   },
   "source": [
    "with open('file1.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    # skip header row\n",
    "    next(reader)\n",
    "    # load remaining data\n",
    "    data = list(reader)\n",
    "print(data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['10', '20', '30'], ['30', '40', '50'], ['60', '70', '80']]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "former-bleeding",
   "metadata": {},
   "source": [
    "We now have a list of lists, and we need to recover the min and max of the numbers in those lists. Notice how the data is actually containing strings, not numbers, so we need to fix that first."
   ]
  },
  {
   "cell_type": "code",
   "id": "hazardous-india",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.652545Z",
     "start_time": "2025-06-18T04:56:35.646035Z"
    }
   },
   "source": [
    "with open('file1.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    # skip header row\n",
    "    next(reader)\n",
    "    # load remaining data\n",
    "    data = list(reader)\n",
    "data = [[float(x) for x in row] for row in data]\n",
    "print(data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.0, 20.0, 30.0], [30.0, 40.0, 50.0], [60.0, 70.0, 80.0]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-diana",
   "metadata": {},
   "source": [
    "Since we'll need to perform this calculation multiple times (and all our csv files contain just rows of numbers), we'll make a function that can do this for us repeatedly:"
   ]
  },
  {
   "cell_type": "code",
   "id": "honey-implement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.733268Z",
     "start_time": "2025-06-18T04:56:35.727283Z"
    }
   },
   "source": [
    "def min_max(data):\n",
    "    row_maxes = [max(row) for row in data]\n",
    "    row_mins = [min(row) for row in data]\n",
    "    \n",
    "    max_ = max(row_maxes)\n",
    "    min_ = min(row_mins)\n",
    "    return min_, max_"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "convinced-popularity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.763254Z",
     "start_time": "2025-06-18T04:56:35.755108Z"
    }
   },
   "source": [
    "min_max(data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 80.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "organized-cleveland",
   "metadata": {},
   "source": [
    "Next we'll look at `file2.csv`"
   ]
  },
  {
   "cell_type": "code",
   "id": "italian-coffee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.811337Z",
     "start_time": "2025-06-18T04:56:35.804977Z"
    }
   },
   "source": [
    "with open('file2.csv') as f:\n",
    "    for row in f:\n",
    "        print(row.strip())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 3.14, 500\n",
      "20, 1, -2\n",
      "-1.1, -2.2, -3.3\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "convertible-portfolio",
   "metadata": {},
   "source": [
    "So this one has no header row."
   ]
  },
  {
   "cell_type": "code",
   "id": "broke-lender",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.893784Z",
     "start_time": "2025-06-18T04:56:35.887096Z"
    }
   },
   "source": [
    "with open('file2.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    # load remaining data\n",
    "    data = list(reader)\n",
    "data = [[float(x) for x in row] for row in data]\n",
    "print(data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 3.14, 500.0], [20.0, 1.0, -2.0], [-1.1, -2.2, -3.3]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "interstate-artwork",
   "metadata": {},
   "source": [
    "And now we can just call our function for min/max:"
   ]
  },
  {
   "cell_type": "code",
   "id": "normal-encyclopedia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:35.960401Z",
     "start_time": "2025-06-18T04:56:35.954294Z"
    }
   },
   "source": [
    "min_max(data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3, 500.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-permit",
   "metadata": {},
   "source": [
    "Let's move on to the third file:"
   ]
  },
  {
   "cell_type": "code",
   "id": "refined-spanking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.045334Z",
     "start_time": "2025-06-18T04:56:36.039126Z"
    }
   },
   "source": [
    "with open('file3.tsv') as f:\n",
    "    for row in f:\n",
    "        print(row.strip())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col1\tcol2\tcol3\n",
      "10\t20\t30\n",
      "40\t50\t60\n",
      "100\t200\t300\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "retired-powder",
   "metadata": {},
   "source": [
    "This one contains a header row, and tab separated values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-graph",
   "metadata": {},
   "source": [
    "If we just try to load it using the default settings for the CSV reader, we won't end up with what we need."
   ]
  },
  {
   "cell_type": "code",
   "id": "northern-association",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.091174Z",
     "start_time": "2025-06-18T04:56:36.084404Z"
    }
   },
   "source": [
    "with open('file3.tsv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header row\n",
    "    # load remaining data\n",
    "    data = list(reader)\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10\\t20\\t30'], ['40\\t50\\t60'], ['100\\t200\\t300']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "technological-reporter",
   "metadata": {},
   "source": [
    "As you can see, the items were not split on the `\\t` character. So, we just need to instruct the CSV reader that `\\t` characters are the item separators using the `delimiter` argument:"
   ]
  },
  {
   "cell_type": "code",
   "id": "refined-picture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.184222Z",
     "start_time": "2025-06-18T04:56:36.175811Z"
    }
   },
   "source": [
    "with open('file3.tsv') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    next(reader)  # skip header row\n",
    "    # load remaining data\n",
    "    data = list(reader)\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10', '20', '30'], ['40', '50', '60'], ['100', '200', '300']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "respective-campbell",
   "metadata": {},
   "source": [
    "We still need our numeroic conversion to happen:"
   ]
  },
  {
   "cell_type": "code",
   "id": "vanilla-liberty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.279290Z",
     "start_time": "2025-06-18T04:56:36.271173Z"
    }
   },
   "source": [
    "data = [[float(x) for x in row] for row in data]\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10.0, 20.0, 30.0], [40.0, 50.0, 60.0], [100.0, 200.0, 300.0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "productive-constant",
   "metadata": {},
   "source": [
    "And now we can find the min/max:"
   ]
  },
  {
   "cell_type": "code",
   "id": "protected-lambda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.370216Z",
     "start_time": "2025-06-18T04:56:36.365088Z"
    }
   },
   "source": [
    "min_max(data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 300.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "narrow-taiwan",
   "metadata": {},
   "source": [
    "Finally,let's look at the last file, `file4.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "dimensional-worcester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.436420Z",
     "start_time": "2025-06-18T04:56:36.429434Z"
    }
   },
   "source": [
    "with open('file4.csv') as f:\n",
    "    for row in f:\n",
    "        print(row.strip())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-col1-|-col2-|-col3-\n",
      "10|20|30\n",
      "-3.14-|-25-|-100-\n",
      "---3.14-|20|-30-\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "flush-digit",
   "metadata": {},
   "source": [
    "Here you can see that we have a header row, the item separators are the pipe character `|`, and the \"quotechar\" is actually `-` - weird, but we can handle that!"
   ]
  },
  {
   "cell_type": "code",
   "id": "prompt-reply",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.484912Z",
     "start_time": "2025-06-18T04:56:36.477827Z"
    }
   },
   "source": [
    "with open('file4.csv') as f:\n",
    "    reader = csv.reader(f, delimiter='|', quotechar='-')\n",
    "    next(reader)  # skip header row\n",
    "    # load remaining data\n",
    "    data = list(reader)\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10', '20', '30'], ['3.14', '25', '100'], ['-3.14', '20', '30']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "minor-weapon",
   "metadata": {},
   "source": [
    "We can then make all these items into numbers and find the min/max:"
   ]
  },
  {
   "cell_type": "code",
   "id": "expressed-westminster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.558591Z",
     "start_time": "2025-06-18T04:56:36.551845Z"
    }
   },
   "source": [
    "data = [[float(x) for x in row] for row in data]\n",
    "min_max(data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.14, 100.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "advanced-teens",
   "metadata": {},
   "source": [
    "You'll notice that the way we handle all these files were as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-peace",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "1. assume that all rowes contain numeric values\n",
    "2. except, possibly, for initial header row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-replication",
   "metadata": {},
   "source": [
    "Approach:\n",
    "1. specify whether we skip header row or not\n",
    "2. specify `delimiter` and `quotechar` optionally\n",
    "3. load data\n",
    "4. convert all items to floats\n",
    "5. find min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-animation",
   "metadata": {},
   "source": [
    "We could actually package all this up into a single function, as long as we allow passing arguments such as `delimiter` and `quotechar` to, ultimately, the csv reader."
   ]
  },
  {
   "cell_type": "code",
   "id": "institutional-arbitration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.629638Z",
     "start_time": "2025-06-18T04:56:36.623476Z"
    }
   },
   "source": [
    "def find_min_max(f_name, has_header_row, **kwargs):\n",
    "    # **kwargs will be passed straight on to the csv reader\n",
    "    with open(f_name) as f:\n",
    "        reader = csv.reader(f, **kwargs)\n",
    "        if has_header_row:\n",
    "            next(reader)  # skip header row\n",
    "        # load remaining data\n",
    "        data = list(reader)\n",
    "    data = [[float(x) for x in row] for row in data]\n",
    "    return min_max(data)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "american-mozambique",
   "metadata": {},
   "source": [
    "We can simplify this a bit:"
   ]
  },
  {
   "cell_type": "code",
   "id": "removed-legend",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.663330Z",
     "start_time": "2025-06-18T04:56:36.658022Z"
    }
   },
   "source": [
    "def find_min_max(f_name, has_header_row=True, **kwargs):\n",
    "    with open(f_name) as f:\n",
    "        reader = csv.reader(f, **kwargs)\n",
    "        if has_header_row:\n",
    "            next(reader)  # skip header row\n",
    "        # load remaining data\n",
    "        data = [[float(x) for x in row] for row in reader]\n",
    "    return min_max(data)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "protective-ultimate",
   "metadata": {},
   "source": [
    "And then we can call it this way:"
   ]
  },
  {
   "cell_type": "code",
   "id": "naked-houston",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.705516Z",
     "start_time": "2025-06-18T04:56:36.698094Z"
    }
   },
   "source": [
    "find_min_max('file1.csv')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 80.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "suburban-noise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.751445Z",
     "start_time": "2025-06-18T04:56:36.745616Z"
    }
   },
   "source": [
    "find_min_max('file2.csv', has_header_row=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3, 500.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "surprising-tucson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.814174Z",
     "start_time": "2025-06-18T04:56:36.807950Z"
    }
   },
   "source": [
    "find_min_max('file3.tsv', delimiter='\\t')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 300.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "enabling-encounter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.898925Z",
     "start_time": "2025-06-18T04:56:36.892416Z"
    }
   },
   "source": [
    "find_min_max('file4.csv', delimiter='|', quotechar='-')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.14, 100.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "fluid-footage",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-court",
   "metadata": {},
   "source": [
    "Given this data structure consisting of a list of dictionaries, write a function that will write this data out to a file, where the column headers (in the first row) are based on the dictionary keys, and the values are flattened out to one row per dictionary (under the corresponding column header)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-victory",
   "metadata": {},
   "source": [
    "Note that not all dictionaries contain all the same keys, nor are the keys necessarily in the same order when present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-decimal",
   "metadata": {},
   "source": [
    "For \"missing\" values, your function should just write an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-rally",
   "metadata": {},
   "source": [
    "For example, given this `data`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "infrared-interpretation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:36.956302Z",
     "start_time": "2025-06-18T04:56:36.950623Z"
    }
   },
   "source": [
    "data = [\n",
    "    {'a': '1_a', 'b': '1_b', 'c': '1_c'},\n",
    "    {'c': '2_c', 'd': '2_d'},\n",
    "    {'a': '3_a', 'c': '3_c', 'e': '3_e'}\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "raw",
   "id": "failing-computer",
   "metadata": {},
   "source": [
    "Your output file should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-authentication",
   "metadata": {},
   "source": [
    "```\n",
    "a,b,c,d,e\n",
    "1_a,1_b,1_c,,,\n",
    ",,2_c,2_d,\n",
    "3_a,,3_c,,3_e\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-outdoors",
   "metadata": {},
   "source": [
    "The order of the columns and rows is not important - as long as they match up with respective column headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-gospel",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-premium",
   "metadata": {},
   "source": [
    "First thing is we need to get the set of all the keys in all the dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "id": "renewable-outline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:37.030329Z",
     "start_time": "2025-06-18T04:56:37.023307Z"
    }
   },
   "source": [
    "keys = {}\n",
    "for d in data:\n",
    "    keys = keys | d.keys()\n",
    "keys"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd', 'e'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "optional-phenomenon",
   "metadata": {},
   "source": [
    "Now we can loop through each dictionary and create a list of all the values for the corresponding keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-rachel",
   "metadata": {},
   "source": [
    "Before we do that however, we want to be sure that the keys will be in the same order, and using a set for the keys doers not guarantee order, so we'll make that into a list first:"
   ]
  },
  {
   "cell_type": "code",
   "id": "public-bowling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:37.124533Z",
     "start_time": "2025-06-18T04:56:37.117608Z"
    }
   },
   "source": [
    "keys = list(keys)\n",
    "keys"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'b', 'a', 'd', 'e']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "bigger-voice",
   "metadata": {},
   "source": [
    "Now we can go ahead and create our list of lists - one list per row, and one value (possibly an empty string), for each item in the row."
   ]
  },
  {
   "cell_type": "code",
   "id": "stone-pierre",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:37.189111Z",
     "start_time": "2025-06-18T04:56:37.182555Z"
    }
   },
   "source": [
    "flattened = []\n",
    "for d in data:\n",
    "    row_list = []\n",
    "    for key in keys:\n",
    "        row_list.append(d.get(key, ''))\n",
    "    flattened.append(row_list)\n",
    "    \n",
    "flattened"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1_c', '1_b', '1_a', '', ''],\n",
       " ['2_c', '', '', '2_d', ''],\n",
       " ['3_c', '', '3_a', '', '3_e']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-hybrid",
   "metadata": {},
   "source": [
    "We can probably use some comprehensions here, let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "id": "happy-habitat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:37.222861Z",
     "start_time": "2025-06-18T04:56:37.215708Z"
    }
   },
   "source": [
    "flattened = []\n",
    "for d in data:\n",
    "    row_list = [d.get(key, '') for key in keys]\n",
    "    flattened.append(row_list)\n",
    "    \n",
    "flattened"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1_c', '1_b', '1_a', '', ''],\n",
       " ['2_c', '', '', '2_d', ''],\n",
       " ['3_c', '', '3_a', '', '3_e']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "random-latter",
   "metadata": {},
   "source": [
    "And one more!"
   ]
  },
  {
   "cell_type": "code",
   "id": "anonymous-south",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:37.321423Z",
     "start_time": "2025-06-18T04:56:37.313845Z"
    }
   },
   "source": [
    "flattened = [[d.get(key, '') for key in keys] for d in data]\n",
    "flattened"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1_c', '1_b', '1_a', '', ''],\n",
       " ['2_c', '', '', '2_d', ''],\n",
       " ['3_c', '', '3_a', '', '3_e']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "radio-matter",
   "metadata": {},
   "source": [
    "And now we could write this to a CSV file using the CSV writer method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-difficulty",
   "metadata": {},
   "source": [
    "Let's go ahead and package all this up, including the CSV writing into a function:"
   ]
  },
  {
   "cell_type": "code",
   "id": "reliable-tactics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:37.447907Z",
     "start_time": "2025-06-18T04:56:37.440162Z"
    }
   },
   "source": [
    "def flatten_to_csv(data, out_file):\n",
    "    keys = {}\n",
    "    for d in data:\n",
    "        keys = keys | d.keys()\n",
    "    keys = list(keys)\n",
    "    flattened = [[d.get(key, '') for key in keys] for d in data]\n",
    "    \n",
    "    with open(out_file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(keys)\n",
    "        for row in flattened:\n",
    "            writer.writerow(row)    "
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "permanent-society",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T04:56:37.499412Z",
     "start_time": "2025-06-18T04:56:37.493638Z"
    }
   },
   "source": [
    "flatten_to_csv(data, 'test.csv')"
   ],
   "outputs": [],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
